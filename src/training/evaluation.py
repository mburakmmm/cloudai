"""
Model Evaluation and Metrics
Model performansÄ±nÄ± Ã¶lÃ§mek iÃ§in metrik hesaplama fonksiyonlarÄ±
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple
import json
import os

try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("âš ï¸ NLTK bulunamadÄ±. BLEU skoru hesaplanamayacak.")

try:
    from evaluate import load
    EVALUATE_AVAILABLE = True
except ImportError:
    EVALUATE_AVAILABLE = False
    print("âš ï¸ evaluate kÃ¼tÃ¼phanesi bulunamadÄ±. BLEU skoru hesaplanamayacak.")


class ModelEvaluator:
    """Model performansÄ±nÄ± deÄŸerlendiren sÄ±nÄ±f"""
    
    def __init__(self, model, tokenizer, device='cpu'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.model.eval()
        
        # Smoothing function for BLEU
        if NLTK_AVAILABLE:
            self.smoothing = SmoothingFunction()
        
        # BLEU metric from evaluate library
        if EVALUATE_AVAILABLE:
            try:
                self.bleu_metric = load("bleu")
            except:
                self.bleu_metric = None
    
    def calculate_perplexity(self, dataloader) -> float:
        """
        Modelin perplexity skorunu hesapla
        
        Args:
            dataloader: Test veri seti
            
        Returns:
            float: Perplexity skoru (dÃ¼ÅŸÃ¼k olmasÄ± daha iyi)
        """
        self.model.eval()
        total_loss = 0.0
        total_tokens = 0
        
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # Forward pass
                outputs = self.model(input_ids, attention_mask)
                
                # Loss hesapla (CrossEntropyLoss ile aynÄ± mantÄ±k)
                logits = outputs.view(-1, outputs.size(-1))
                labels_flat = labels.view(-1)
                
                # Sadece geÃ§erli label'larÄ± hesapla (-100 olmayan)
                valid_mask = labels_flat != -100
                if valid_mask.sum() > 0:
                    valid_logits = logits[valid_mask]
                    valid_labels = labels_flat[valid_mask]
                    
                    loss = F.cross_entropy(valid_logits, valid_labels, reduction='sum')
                    total_loss += loss.item()
                    total_tokens += valid_mask.sum().item()
        
        if total_tokens == 0:
            return float('inf')
        
        avg_loss = total_loss / total_tokens
        perplexity = torch.exp(torch.tensor(avg_loss)).item()
        
        return perplexity
    
    def calculate_bleu(self, generated_texts: List[str], reference_texts: List[str]) -> Dict[str, float]:
        """
        BLEU skorunu hesapla
        
        Args:
            generated_texts: Model tarafÄ±ndan Ã¼retilen metinler
            reference_texts: Referans (gerÃ§ek) metinler
            
        Returns:
            Dict: BLEU skorlarÄ±
        """
        results = {}
        
        # NLTK ile BLEU hesaplama
        if NLTK_AVAILABLE:
            try:
                bleu_scores = []
                for gen, ref in zip(generated_texts, reference_texts):
                    # Tokenize (basit whitespace split)
                    gen_tokens = gen.split()
                    ref_tokens = ref.split()
                    
                    # BLEU hesapla
                    score = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=self.smoothing.method1)
                    bleu_scores.append(score)
                
                results['bleu_nltk'] = np.mean(bleu_scores)
                results['bleu_nltk_std'] = np.std(bleu_scores)
            except Exception as e:
                print(f"âš ï¸ NLTK BLEU hesaplama hatasÄ±: {e}")
                results['bleu_nltk'] = 0.0
        
        # Evaluate library ile BLEU hesaplama
        if EVALUATE_AVAILABLE and self.bleu_metric:
            try:
                # BLEU iÃ§in referans metinleri list of list formatÄ±nda olmalÄ±
                references = [[ref] for ref in reference_texts]
                
                bleu_result = self.bleu_metric.compute(
                    predictions=generated_texts,
                    references=references
                )
                
                results['bleu_evaluate'] = bleu_result['bleu']
            except Exception as e:
                print(f"âš ï¸ Evaluate BLEU hesaplama hatasÄ±: {e}")
                results['bleu_evaluate'] = 0.0
        
        return results
    
    def calculate_accuracy(self, dataloader) -> float:
        """
        Token-level accuracy hesapla
        
        Args:
            dataloader: Test veri seti
            
        Returns:
            float: Accuracy skoru
        """
        self.model.eval()
        correct_tokens = 0
        total_tokens = 0
        
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # Forward pass
                outputs = self.model(input_ids, attention_mask)
                
                # Predictions
                predictions = torch.argmax(outputs, dim=-1)
                
                # Sadece geÃ§erli label'larÄ± hesapla
                valid_mask = labels != -100
                if valid_mask.sum() > 0:
                    valid_preds = predictions[valid_mask]
                    valid_labels = labels[valid_mask]
                    
                    correct_tokens += (valid_preds == valid_labels).sum().item()
                    total_tokens += valid_mask.sum().item()
        
        if total_tokens == 0:
            return 0.0
        
        accuracy = correct_tokens / total_tokens
        return accuracy
    
    def generate_sample_responses(self, prompts: List[str], max_length: int = 50) -> List[str]:
        """
        Test prompt'larÄ± iÃ§in Ã¶rnek cevaplar Ã¼ret
        
        Args:
            prompts: Test prompt'larÄ±
            max_length: Maksimum token sayÄ±sÄ±
            
        Returns:
            List[str]: Ãœretilen cevaplar
        """
        self.model.eval()
        responses = []
        
        with torch.no_grad():
            for prompt in prompts:
                # Prompt'u tokenize et
                inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
                
                # Generation
                outputs = self.model.generate(
                    inputs,
                    max_length=max_length,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
                
                # Decode
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                responses.append(response)
        
        return responses
    
    def comprehensive_evaluation(self, dataloader, test_prompts: Optional[List[str]] = None) -> Dict[str, float]:
        """
        KapsamlÄ± model deÄŸerlendirmesi
        
        Args:
            dataloader: Test veri seti
            test_prompts: Test prompt'larÄ± (BLEU iÃ§in)
            
        Returns:
            Dict: TÃ¼m metrikler
        """
        print("ğŸ” Model deÄŸerlendirmesi baÅŸlÄ±yor...")
        
        metrics = {}
        
        # Perplexity
        print("ğŸ“Š Perplexity hesaplanÄ±yor...")
        metrics['perplexity'] = self.calculate_perplexity(dataloader)
        
        # Accuracy
        print("ğŸ“Š Accuracy hesaplanÄ±yor...")
        metrics['accuracy'] = self.calculate_accuracy(dataloader)
        
        # BLEU (eÄŸer test prompt'larÄ± verildiyse)
        if test_prompts:
            print("ğŸ“Š BLEU skoru hesaplanÄ±yor...")
            # Ã–rnek cevaplar Ã¼ret
            generated_responses = self.generate_sample_responses(test_prompts)
            
            # Referans cevaplar (gerÃ§ek hayatta veri setinden gelir)
            # Åimdilik basit bir Ã¶rnek
            reference_responses = [f"Bu bir test cevabÄ±dÄ±r: {prompt}" for prompt in test_prompts]
            
            bleu_scores = self.calculate_bleu(generated_responses, reference_responses)
            metrics.update(bleu_scores)
        
        print("âœ… Model deÄŸerlendirmesi tamamlandÄ±!")
        return metrics
    
    def save_evaluation_results(self, results: Dict[str, float], filepath: str):
        """
        DeÄŸerlendirme sonuÃ§larÄ±nÄ± kaydet
        
        Args:
            results: DeÄŸerlendirme sonuÃ§larÄ±
            filepath: KayÄ±t dosya yolu
        """
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ DeÄŸerlendirme sonuÃ§larÄ± kaydedildi: {filepath}")


def calculate_perplexity(model, dataloader) -> float:
    """Standalone perplexity hesaplama fonksiyonu"""
    evaluator = ModelEvaluator(model, None, device=next(model.parameters()).device)
    return evaluator.calculate_perplexity(dataloader)


def calculate_bleu(model, dataset, tokenizer) -> Dict[str, float]:
    """Standalone BLEU hesaplama fonksiyonu"""
    evaluator = ModelEvaluator(model, tokenizer, device=next(model.parameters()).device)
    
    # Basit test prompt'larÄ±
    test_prompts = [
        "Merhaba, nasÄ±lsÄ±n?",
        "Hava nasÄ±l?",
        "Python nedir?",
        "Makine Ã¶ÄŸrenmesi hakkÄ±nda bilgi ver"
    ]
    
    generated_responses = evaluator.generate_sample_responses(test_prompts)
    reference_responses = [f"Test cevabÄ±: {prompt}" for prompt in test_prompts]
    
    return evaluator.calculate_bleu(generated_responses, reference_responses)


if __name__ == "__main__":
    # Test
    print("ğŸ§ª Evaluation modÃ¼lÃ¼ test ediliyor...")
    print("âœ… ModÃ¼l baÅŸarÄ±yla yÃ¼klendi!")
