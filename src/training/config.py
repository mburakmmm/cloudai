"""
Training Configuration
EÄŸitim iÃ§in gerekli tÃ¼m konfigÃ¼rasyon parametreleri
"""

import os
from typing import Dict, Any


def get_model_config() -> Dict[str, Any]:
    """Model konfigÃ¼rasyonu"""
    return {
        "vocab_size": 8000,  # 30000 â†’ 8000 (daha iyi Ã¶ÄŸrenme)
        "d_model": 512,
        "nhead": 8,
        "num_decoder_layers": 6,
        "dim_feedforward": 2048,
        "max_seq_length": 512,
        "dropout": 0.1
    }


def get_training_config() -> Dict[str, Any]:
    """EÄŸitim konfigÃ¼rasyonu"""
    return {
        "learning_rate": 1e-4,  # 2e-4 â†’ 1e-4 (daha stabil)
        "weight_decay": 0.01,
        "batch_size": 16,  # 32 â†’ 16 (daha iyi Ã¶ÄŸrenme)
        "num_epochs": 50,  # 10 â†’ 50 (5x daha fazla eÄŸitim)
        "max_seq_length": 256,
        "warmup_steps": 500,  # 100 â†’ 500 (daha uzun warmup)
        "gradient_clip_val": 1.0,
        "early_stopping_patience": 10,  # 5 â†’ 10 (daha sabÄ±rlÄ±)
        "save_every": 5,  # 1 â†’ 5 (daha az checkpoint)
        "gradient_accumulation_steps": 4,  # 2 â†’ 4 (daha bÃ¼yÃ¼k effective batch)
        "eval_every": 5  # 2 â†’ 5 (daha az validation)
    }


def get_paths_config() -> Dict[str, Any]:
    """Dosya yollarÄ± konfigÃ¼rasyonu"""
    return {
        "data": None,  # VeritabanÄ±ndan Ã§ekilecek
        "model_save_path": "models/my_chatbot.pth",
        "tokenizer_path": "models/tokenizer",
        "checkpoint_dir": "checkpoints",
        "logs_dir": "logs",
        "output_dir": "outputs"
    }


def get_hardware_config() -> Dict[str, Any]:
    """DonanÄ±m konfigÃ¼rasyonu"""
    return {
        "device": "auto",  # auto, cuda, cpu, mps
        "num_workers": 8,  # 4 â†’ 8 (2x paralel iÅŸlem)
        "pin_memory": True,
        "mixed_precision": True,  # False â†’ True (FP16, 2x hÄ±z)
        "compile_model": True  # Yeni: PyTorch 2.0+ compile
    }


def get_optimizer_config() -> Dict[str, Any]:
    """Optimizer konfigÃ¼rasyonu"""
    return {
        "type": "adamw",  # adamw, adam, sgd
        "betas": (0.9, 0.999),
        "eps": 1e-8,
        "amsgrad": False
    }


def get_scheduler_config() -> Dict[str, Any]:
    """Scheduler konfigÃ¼rasyonu"""
    return {
        "type": "cosine_with_warmup",  # cosine_with_warmup, onecycle, step
        "warmup_steps": 100,
        "min_lr": 1e-6
    }


def get_dataloader_config() -> Dict[str, Any]:
    """DataLoader konfigÃ¼rasyonu"""
    return {
        "shuffle": True,
        "drop_last": True,
        "pin_memory": True,
        "persistent_workers": True,  # Yeni: Worker'larÄ± yeniden baÅŸlatma
        "prefetch_factor": 2  # Yeni: Veri Ã¶nceden yÃ¼kleme
    }


def get_validation_config() -> Dict[str, Any]:
    """Validation konfigÃ¼rasyonu"""
    return {
        "val_split": 0.1,
        "eval_batch_size": 16,
        "eval_every": 1
    }


def get_logging_config() -> Dict[str, Any]:
    """Logging konfigÃ¼rasyonu"""
    return {
        "log_every_n_steps": 10,
        "save_every_n_epochs": 1,
        "tensorboard": False,
        "wandb": False
    }


def get_config() -> Dict[str, Any]:
    """Ana konfigÃ¼rasyon"""
    return {
        "model": get_model_config(),
        "training": get_training_config(),
        "paths": get_paths_config(),
        "hardware": get_hardware_config(),
        "optimizer": get_optimizer_config(),
        "scheduler": get_scheduler_config(),
        "dataloader": get_dataloader_config(),
        "validation": get_validation_config(),
        "logging": get_logging_config()
    }


def create_directories():
    """Gerekli dizinleri oluÅŸtur"""
    config = get_config()
    
    dirs_to_create = [
        config["paths"]["checkpoint_dir"],
        config["paths"]["logs_dir"],
        config["paths"]["output_dir"],
        os.path.dirname(config["paths"]["model_save_path"]),
        config["paths"]["tokenizer_path"]
    ]
    
    for dir_path in dirs_to_create:
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)
            print(f"ğŸ“ Dizin oluÅŸturuldu: {dir_path}")


def print_config_summary():
    """KonfigÃ¼rasyon Ã¶zetini yazdÄ±r"""
    config = get_config()
    
    print("ğŸ”§ KonfigÃ¼rasyon Ã–zeti:")
    print("=" * 50)
    
    print(f"ğŸ“Š Model: {config['model']['d_model']}D, {config['model']['num_decoder_layers']} layers")
    print(f"ğŸ¯ EÄŸitim: {config['training']['num_epochs']} epoch, lr={config['training']['learning_rate']}")
    print(f"ğŸ“¦ Batch: {config['training']['batch_size']}, Seq Len: {config['training']['max_seq_length']}")
    print(f"ğŸ’¾ KayÄ±t: {config['paths']['model_save_path']}")
    print(f"ğŸ”§ Optimizer: {config['optimizer']['type'].upper()}")
    print(f"ğŸ“ˆ Scheduler: {config['scheduler']['type']}")
    print(f"ğŸ’» Cihaz: {config['hardware']['device']}")


if __name__ == "__main__":
    # Test
    create_directories()
    print_config_summary()
