"""
Training Configuration
EÄŸitim iÃ§in gerekli tÃ¼m konfigÃ¼rasyon parametreleri
"""

import os
from typing import Dict, Any


def get_model_config() -> Dict[str, Any]:
    """Model konfigÃ¼rasyonu - 17K+ veri iÃ§in optimize edildi"""
    return {
        "vocab_size": 50000,  # Tokenizer ile uyumlu (17K+ veri iÃ§in)
        "d_model": 768,  # 512 â†’ 768 (daha bÃ¼yÃ¼k model)
        "nhead": 12,  # 8 â†’ 12 (attention baÅŸlÄ±k sayÄ±sÄ±)
        "num_decoder_layers": 8,  # 6 â†’ 8 (daha derin model)
        "dim_feedforward": 3072,  # 2048 â†’ 3072 (daha geniÅŸ FFN)
        "max_seq_length": 512,
        "dropout": 0.1
    }


def get_training_config() -> Dict[str, Any]:
    """EÄŸitim konfigÃ¼rasyonu - 17K+ veri iÃ§in optimize edildi"""
    return {
        "learning_rate": 5e-5,  # 1e-4 â†’ 5e-5 (daha stabil eÄŸitim)
        "weight_decay": 0.01,
        "batch_size": 16,  # 32 â†’ 16 (daha bÃ¼yÃ¼k model iÃ§in)
        "num_epochs": 30,  # 25 â†’ 30 (daha uzun eÄŸitim)
        "max_seq_length": 256,
        "warmup_steps": 1500,  # 1000 â†’ 1500 (daha uzun warmup)
        "gradient_clip_val": 1.0,
        "early_stopping_patience": 10,  # 8 â†’ 10 (daha sabÄ±rlÄ±)
        "save_every": 2,  # 3 â†’ 2 (daha sÄ±k checkpoint)
        "gradient_accumulation_steps": 4,  # 2 â†’ 4 (effective batch = 64)
        "eval_every": 1,  # 2 â†’ 1 (her epoch'ta validation)
        "validation_split": 0.20,  # 0.15 â†’ 0.20 (%20 validation)
        "lr_scheduler": "cosine_with_warmup",  # Yeni: Scheduler tipi
        "min_lr": 1e-6  # Yeni: Minimum learning rate
    }


def get_paths_config() -> Dict[str, Any]:
    """Dosya yollarÄ± konfigÃ¼rasyonu"""
    return {
        "data": None,  # VeritabanÄ±ndan Ã§ekilecek
        "model_save_path": "models/my_chatbot.pth",
        "tokenizer_path": "models/tokenizer",
        "checkpoint_dir": "checkpoints",
        "logs_dir": "logs",
        "output_dir": "outputs"
    }


def get_hardware_config() -> Dict[str, Any]:
    """DonanÄ±m konfigÃ¼rasyonu - 17K+ veri iÃ§in optimize edildi"""
    return {
        "device": "auto",  # auto, cuda, cpu, mps
        "num_workers": 4,  # 8 â†’ 4 (daha bÃ¼yÃ¼k model iÃ§in)
        "pin_memory": True,
        "mixed_precision": True,  # FP16, 2x hÄ±z
        "compile_model": True,  # PyTorch 2.0+ compile
        "gradient_checkpointing": True,  # Yeni: Memory tasarrufu
        "use_amp": True,  # Yeni: Automatic Mixed Precision
        "dtype": "float16"  # Yeni: Default data type
    }


def get_optimizer_config() -> Dict[str, Any]:
    """Optimizer konfigÃ¼rasyonu"""
    return {
        "type": "adamw",  # adamw, adam, sgd
        "betas": (0.9, 0.999),
        "eps": 1e-8,
        "amsgrad": False
    }


def get_scheduler_config() -> Dict[str, Any]:
    """Scheduler konfigÃ¼rasyonu"""
    return {
        "type": "cosine_with_warmup",  # cosine_with_warmup, onecycle, step
        "warmup_steps": 100,
        "min_lr": 1e-6
    }


def get_dataloader_config() -> Dict[str, Any]:
    """DataLoader konfigÃ¼rasyonu"""
    return {
        "shuffle": True,
        "drop_last": True,
        "pin_memory": True,
        "persistent_workers": True,  # Yeni: Worker'larÄ± yeniden baÅŸlatma
        "prefetch_factor": 2  # Yeni: Veri Ã¶nceden yÃ¼kleme
    }


def get_validation_config() -> Dict[str, Any]:
    """Validation konfigÃ¼rasyonu"""
    return {
        "val_split": 0.1,
        "eval_batch_size": 16,
        "eval_every": 1
    }


def get_logging_config() -> Dict[str, Any]:
    """Logging konfigÃ¼rasyonu"""
    return {
        "log_every_n_steps": 10,
        "save_every_n_epochs": 1,
        "tensorboard": False,
        "wandb": False
    }


def get_config() -> Dict[str, Any]:
    """Ana konfigÃ¼rasyon"""
    return {
        "model": get_model_config(),
        "training": get_training_config(),
        "paths": get_paths_config(),
        "hardware": get_hardware_config(),
        "optimizer": get_optimizer_config(),
        "scheduler": get_scheduler_config(),
        "dataloader": get_dataloader_config(),
        "validation": get_validation_config(),
        "logging": get_logging_config()
    }


def create_directories():
    """Gerekli dizinleri oluÅŸtur"""
    config = get_config()
    
    dirs_to_create = [
        config["paths"]["checkpoint_dir"],
        config["paths"]["logs_dir"],
        config["paths"]["output_dir"],
        os.path.dirname(config["paths"]["model_save_path"]),
        config["paths"]["tokenizer_path"]
    ]
    
    for dir_path in dirs_to_create:
        if dir_path:
            os.makedirs(dir_path, exist_ok=True)
            print(f"ğŸ“ Dizin oluÅŸturuldu: {dir_path}")


def print_config_summary():
    """KonfigÃ¼rasyon Ã¶zetini yazdÄ±r"""
    config = get_config()
    
    print("ğŸ”§ KonfigÃ¼rasyon Ã–zeti:")
    print("=" * 50)
    
    print(f"ğŸ“Š Model: {config['model']['d_model']}D, {config['model']['num_decoder_layers']} layers")
    print(f"ğŸ¯ EÄŸitim: {config['training']['num_epochs']} epoch, lr={config['training']['learning_rate']}")
    print(f"ğŸ“¦ Batch: {config['training']['batch_size']}, Seq Len: {config['training']['max_seq_length']}")
    print(f"ğŸ’¾ KayÄ±t: {config['paths']['model_save_path']}")
    print(f"ğŸ”§ Optimizer: {config['optimizer']['type'].upper()}")
    print(f"ğŸ“ˆ Scheduler: {config['scheduler']['type']}")
    print(f"ğŸ’» Cihaz: {config['hardware']['device']}")


if __name__ == "__main__":
    # Test
    create_directories()
    print_config_summary()
