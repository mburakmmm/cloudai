"""
Custom Tokenizer Implementation
Hugging Face transformers kÃ¼tÃ¼phanesi kullanarak BPE tokenizer
"""

import os
import json
from typing import List, Dict, Optional
from transformers import PreTrainedTokenizerFast, AutoTokenizer
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors


class CustomTokenizer:
    """Ã–zel tokenizer sÄ±nÄ±fÄ± - BPE tabanlÄ±"""
    
    def __init__(self, vocab_size: int = 50000, min_frequency: int = 3):
        """
        Tokenizer baÅŸlat
        
        Args:
            vocab_size: Kelime hazinesi boyutu (17K+ veri iÃ§in optimize edildi)
            min_frequency: Minimum token frekansÄ± (daha az gÃ¼rÃ¼ltÃ¼ iÃ§in)
        """
        self.vocab_size = vocab_size
        self.min_frequency = min_frequency
        self.tokenizer = None
        self.is_trained = False
        
    def train(self, texts: List[str], save_path: Optional[str] = None) -> None:
        """Metinler Ã¼zerinden tokenizer'Ä± eÄŸit"""
        print(f"ðŸ”§ Tokenizer eÄŸitimi baÅŸlÄ±yor... {len(texts)} metin ile")
        
        # BPE tokenizer oluÅŸtur
        tokenizer = Tokenizer(models.BPE())
        
        # Pre-tokenizer: ByteLevel
        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)
        
        # Decoder: ByteLevel
        tokenizer.decoder = decoders.ByteLevel()
        
        # Post-processor: RobertaLM
        tokenizer.post_processor = processors.RobertaProcessing(
            sep=("</s>", 2),
            cls=("<s>", 1)
        )
        
        # Trainer - 17K+ veri iÃ§in optimize edildi
        trainer = trainers.BpeTrainer(
            vocab_size=self.vocab_size,
            min_frequency=self.min_frequency,
            special_tokens=["<s>", "</s>", "<unk>", "<pad>", "<mask>"],
            show_progress=True,
            limit_alphabet=1000,  # Alfabe boyutunu sÄ±nÄ±rla
            initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # ByteLevel alfabesi
        )
        
        # EÄŸitim
        tokenizer.train_from_iterator(texts, trainer=trainer)
        
        # PreTrainedTokenizerFast'e Ã§evir
        self.tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token="<s>",
            eos_token="</s>",
            unk_token="<unk>",
            pad_token="<pad>",
            cls_token="<s>",
            sep_token="</s>",
            mask_token="<mask>"
        )
        
        self.is_trained = True
        print(f"âœ… Tokenizer eÄŸitimi tamamlandÄ±! Vocab size: {self.tokenizer.vocab_size}")
        
        # Kaydet
        if save_path:
            self.save(save_path)
    
    def save(self, path: str) -> None:
        """Tokenizer'Ä± dosyaya kaydet"""
        if not self.is_trained:
            raise ValueError("Tokenizer henÃ¼z eÄŸitilmemiÅŸ!")
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        self.tokenizer.save_pretrained(path)
        print(f"ðŸ’¾ Tokenizer kaydedildi: {path}")
    
    def load(self, path: str) -> None:
        """Tokenizer'Ä± dosyadan yÃ¼kle"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(path)
            self.is_trained = True
            print(f"ðŸ“‚ Tokenizer yÃ¼klendi: {path}")
        except Exception as e:
            print(f"âŒ Tokenizer yÃ¼kleme hatasÄ±: {e}")
            raise
    
    def get_tokenizer(self) -> PreTrainedTokenizerFast:
        """EÄŸitilmiÅŸ tokenizer'Ä± dÃ¶ndÃ¼r"""
        if not self.is_trained:
            raise ValueError("Tokenizer henÃ¼z eÄŸitilmemiÅŸ!")
        return self.tokenizer
    
    @property
    def pad_token_id(self) -> int:
        """Pad token ID'sini dÃ¶ndÃ¼r"""
        if not self.is_trained:
            return 0
        return self.tokenizer.pad_token_id or 0
    
    @property
    def eos_token_id(self) -> int:
        """EOS token ID'sini dÃ¶ndÃ¼r"""
        if not self.is_trained:
            return 1
        return self.tokenizer.eos_token_id or 1
    
    @property
    def bos_token_id(self) -> int:
        """BOS token ID'sini dÃ¶ndÃ¼r"""
        if not self.is_trained:
            return 1
        return self.tokenizer.bos_token_id or 1
    
    @property
    def unk_token_id(self) -> int:
        """UNK token ID'sini dÃ¶ndÃ¼r"""
        if not self.is_trained:
            return 3
        return self.tokenizer.unk_token_id or 3
        if not self.is_trained:
            raise ValueError("Tokenizer henÃ¼z eÄŸitilmemiÅŸ!")
        return self.tokenizer
    
    def encode(self, text: str, max_length: Optional[int] = None) -> Dict:
        """Metni tokenize et"""
        if not self.is_trained:
            raise ValueError("Tokenizer henÃ¼z eÄŸitilmemiÅŸ!")
        
        return self.tokenizer(
            text,
            max_length=max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
    
    def decode(self, token_ids: List[int]) -> str:
        """Token ID'leri metne Ã§evir"""
        if not self.is_trained:
            raise ValueError("Tokenizer henÃ¼z eÄŸitilmemiÅŸ!")
        
        return self.tokenizer.decode(token_ids, skip_special_tokens=True)
    
    def get_vocab_size(self) -> int:
        """Vocabulary boyutunu dÃ¶ndÃ¼r"""
        if not self.is_trained:
            return 0
        return self.tokenizer.vocab_size
